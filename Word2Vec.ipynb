{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc():\n",
    "    new_text=[]\n",
    "    words=[]\n",
    "    sentences=[]\n",
    "    \n",
    "    text=open('corpus_text.txt').read().lower()\n",
    "    raw_text=nltk.sent_tokenize(text)\n",
    "    for sent in raw_text:\n",
    "        for char in sent:\n",
    "            if char in '!?.^' :\n",
    "                sent=sent.replace(char,'')\n",
    "        new_text.append(sent)\n",
    "    \n",
    "    for sent in new_text:\n",
    "        for word in sent.split():\n",
    "            words.append(word)\n",
    "         \n",
    "    words=set(words)       \n",
    "    word2int,int2word = {},{}\n",
    "    vocab_size = len(words) \n",
    "\n",
    "    for i,word in enumerate(words):\n",
    "        word2int[word] = i\n",
    "        int2word[i] = word\n",
    "        \n",
    "    for sentence in new_text:\n",
    "        sentences.append(sentence.split())\n",
    "       \n",
    "    return(word2int,int2word,sentences,vocab_size,words)\n",
    "    \n",
    "def make_training_data(sentences):\n",
    "    WINDOW_SIZE = 2\n",
    "    data=[]\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for word_index, word in enumerate(sentence):\n",
    "            for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n",
    "                if nb_word != word:\n",
    "                    data.append([word, nb_word])  \n",
    "    return(data)\n",
    "                    \n",
    "def to_one_hot(data_point_index,vocab_size):\n",
    "    temp=np.zeros(vocab_size)\n",
    "    temp[data_point_index]=1\n",
    "    return temp\n",
    "\n",
    "def make_vectors(data,word2int,vocab_size):\n",
    "    x_train=[]#input_word\n",
    "    y_train=[]#output_word\n",
    "\n",
    "    for data_word in data:\n",
    "        x_train.append(to_one_hot(word2int[data_word[0]],vocab_size))\n",
    "        y_train.append(to_one_hot(word2int[data_word[1]],vocab_size))\n",
    "    \n",
    "    x_train=np.asarray(x_train)\n",
    "    y_train=np.asarray(y_train)\n",
    "    \n",
    "    return(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_results(x_train,y_train):\n",
    "    print(\"X_train:\", x_train)\n",
    "    print(\"Y_train:\", y_train)\n",
    "    print(x_train.shape,y_train.shape)\n",
    "    \n",
    "def create_and_train_model(x_train,y_train,vocab_size):\n",
    "    x=tf.placeholder(tf.float32,shape=(None,vocab_size))\n",
    "    y_label=tf.placeholder(tf.float32,shape=(None,vocab_size))\n",
    "\n",
    "    EMBEDDING_DIM=5\n",
    "    W1=tf.Variable(tf.random_normal([vocab_size,EMBEDDING_DIM]))\n",
    "    b1=tf.Variable(tf.random_normal([EMBEDDING_DIM]))\n",
    "    hidden_representation=tf.add(tf.matmul(x,W1),b1)\n",
    "    W2=tf.Variable(tf.random_normal([EMBEDDING_DIM,vocab_size]))\n",
    "    b2=tf.Variable(tf.random_normal([vocab_size]))\n",
    "\n",
    "    prediction=tf.nn.softmax(tf.add(tf.matmul(hidden_representation,W2),b2))\n",
    "    \n",
    "    sess=tf.Session()\n",
    "    init=tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    #define loss\n",
    "    cross_entropy_loss=tf.reduce_mean(-tf.reduce_sum(y_label*tf.log(prediction),reduction_indices=[1]))\n",
    "    #start training\n",
    "    train_step=tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\n",
    "\n",
    "    n_iters=100000\n",
    "    for i in range(n_iters):\n",
    "        sess.run(train_step,feed_dict={x:x_train,y_label:y_train})\n",
    "        sess.run(cross_entropy_loss,feed_dict={x:x_train,y_label:y_train})\n",
    "        if(i%500==0):\n",
    "            print('Loss after {}th iteration is : {}'.format(i,sess.run(cross_entropy_loss,feed_dict={x:x_train,y_label:y_train})))\n",
    "            \n",
    "    vectors=sess.run(W1+b1)\n",
    "    #print(sess.run(W1))\n",
    "    #print(vectors)\n",
    "    return(vectors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vec1,vec2):\n",
    "    return np.sqrt(np.sum(vec1-vec2)**2)\n",
    "\n",
    "def find_closest(word_index,vectors):\n",
    "    min_dist=10000\n",
    "    min_index=-1\n",
    "    query_vector=vectors[word_index]\n",
    "    for index,vector in enumerate(vectors):\n",
    "        if euclidean_distance(vector,query_vector)<min_dist and not np.array_equal(vector,query_vector):\n",
    "            min_dist=euclidean_distance(vector,query_vector)\n",
    "            min_index=index\n",
    "            \n",
    "    return min_index    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(vectors,words,word2int):\n",
    "    model=TSNE(n_components=2,random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    vectors=model.fit_transform(vectors)\n",
    "    normalizer=preprocessing.Normalizer()\n",
    "    vectors=normalizer.fit_transform(vectors,'l2')\n",
    "    \n",
    "    fig,ax=plt.subplots()\n",
    "    for word in words:\n",
    "        print(word,vectors[word2int[word]][1])\n",
    "        ax.annotate(word,(vectors[word2int[word]][0],vectors[word2int[word]][1]))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    word2int,int2word,sentences,vocab_size,words=read_doc()\n",
    "    data=make_training_data(sentences)\n",
    "    x_train,y_train=make_vectors(data,word2int,vocab_size)\n",
    "    \n",
    "    #view_results(x_train,y_train)\n",
    "    \n",
    "    vectors=create_and_train_model(x_train,y_train,vocab_size)\n",
    "    \n",
    "    print('Word closest to sister : ',int2word[find_closest(word2int['sister'], vectors)])\n",
    "    print('Word closest to monica : ',int2word[find_closest(word2int['monica'], vectors)])\n",
    "    print('Word closest to eat : ',int2word[find_closest(word2int['eat'], vectors)])\n",
    "    \n",
    "    plt.xticks(range(-1.5,1.5))\n",
    "    plt.yticks(range(-1.5,1.5))\n",
    "    plot(vectors,words,word2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0th iteration is : 9.192723274230957\n",
      "Loss after 500th iteration is : 6.555208683013916\n",
      "Loss after 1000th iteration is : 5.929463863372803\n",
      "Loss after 1500th iteration is : 5.591761589050293\n",
      "Loss after 2000th iteration is : 5.371921062469482\n",
      "Loss after 2500th iteration is : 5.212832450866699\n",
      "Loss after 3000th iteration is : 5.08939266204834\n",
      "Loss after 3500th iteration is : 4.98776388168335\n",
      "Loss after 4000th iteration is : 4.900006294250488\n",
      "Loss after 4500th iteration is : 4.8215718269348145\n",
      "Loss after 5000th iteration is : 4.7497239112854\n",
      "Loss after 5500th iteration is : 4.682715892791748\n",
      "Loss after 6000th iteration is : 4.6193976402282715\n",
      "Loss after 6500th iteration is : 4.558995723724365\n",
      "Loss after 7000th iteration is : 4.500997066497803\n",
      "Loss after 7500th iteration is : 4.445059776306152\n",
      "Loss after 8000th iteration is : 4.390960216522217\n",
      "Loss after 8500th iteration is : 4.338551044464111\n",
      "Loss after 9000th iteration is : 4.287731647491455\n",
      "Loss after 9500th iteration is : 4.238432884216309\n",
      "Loss after 10000th iteration is : 4.1906046867370605\n",
      "Loss after 10500th iteration is : 4.144207000732422\n",
      "Loss after 11000th iteration is : 4.099208831787109\n",
      "Loss after 11500th iteration is : 4.0555853843688965\n",
      "Loss after 12000th iteration is : 4.013319492340088\n",
      "Loss after 12500th iteration is : 3.972395181655884\n",
      "Loss after 13000th iteration is : 3.9328017234802246\n",
      "Loss after 13500th iteration is : 3.8945281505584717\n",
      "Loss after 14000th iteration is : 3.8575637340545654\n",
      "Loss after 14500th iteration is : 3.821897506713867\n",
      "Loss after 15000th iteration is : 3.787517547607422\n",
      "Loss after 15500th iteration is : 3.754406213760376\n",
      "Loss after 16000th iteration is : 3.7225446701049805\n",
      "Loss after 16500th iteration is : 3.6919069290161133\n",
      "Loss after 17000th iteration is : 3.6624650955200195\n",
      "Loss after 17500th iteration is : 3.6341843605041504\n",
      "Loss after 18000th iteration is : 3.6070289611816406\n",
      "Loss after 18500th iteration is : 3.580960273742676\n",
      "Loss after 19000th iteration is : 3.5559396743774414\n",
      "Loss after 19500th iteration is : 3.5319252014160156\n",
      "Loss after 20000th iteration is : 3.508876085281372\n",
      "Loss after 20500th iteration is : 3.4867494106292725\n",
      "Loss after 21000th iteration is : 3.4655075073242188\n",
      "Loss after 21500th iteration is : 3.445108652114868\n",
      "Loss after 22000th iteration is : 3.4255149364471436\n",
      "Loss after 22500th iteration is : 3.4066896438598633\n",
      "Loss after 23000th iteration is : 3.388597249984741\n",
      "Loss after 23500th iteration is : 3.3712053298950195\n",
      "Loss after 24000th iteration is : 3.354482412338257\n",
      "Loss after 24500th iteration is : 3.3383982181549072\n",
      "Loss after 25000th iteration is : 3.322927236557007\n",
      "Loss after 25500th iteration is : 3.308041572570801\n",
      "Loss after 26000th iteration is : 3.29371976852417\n",
      "Loss after 26500th iteration is : 3.2799344062805176\n",
      "Loss after 27000th iteration is : 3.266665458679199\n",
      "Loss after 27500th iteration is : 3.2538890838623047\n",
      "Loss after 28000th iteration is : 3.241583824157715\n",
      "Loss after 28500th iteration is : 3.2297282218933105\n",
      "Loss after 29000th iteration is : 3.2183024883270264\n",
      "Loss after 29500th iteration is : 3.2072839736938477\n",
      "Loss after 30000th iteration is : 3.1966543197631836\n",
      "Loss after 30500th iteration is : 3.186394214630127\n",
      "Loss after 31000th iteration is : 3.176485538482666\n",
      "Loss after 31500th iteration is : 3.166910171508789\n",
      "Loss after 32000th iteration is : 3.157651424407959\n",
      "Loss after 32500th iteration is : 3.148693323135376\n",
      "Loss after 33000th iteration is : 3.1400198936462402\n",
      "Loss after 33500th iteration is : 3.1316163539886475\n",
      "Loss after 34000th iteration is : 3.1234679222106934\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
